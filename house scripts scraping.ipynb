{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#libraries\n",
    "import urllib2\n",
    "from unidecode import unidecode\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from unidecode import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#open episode lists page, put it in the beautiful soup\n",
    "episodes_index = \"http://www.clinic-duty.livejournal.com/12225.html\"\n",
    "episodes_index_pages = urllib2.urlopen(episodes_index)\n",
    "episodes_index_soup = BeautifulSoup(episodes_index_pages, \"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get the links on the tables (it's on the tables)\n",
    "table = episodes_index_soup.find('table')\n",
    "episodes_url = []\n",
    "#get those contain certain pattern\n",
    "all_episodes_links = table.find_all('a', attrs={'href': re.compile(\"\\/community\\.livejournal\\.com\\/clinic_duty\\/[0-9]+\\.html\")})\n",
    "#get the href\n",
    "for link in all_episodes_links:\n",
    "    episodes_url.append(link.get(\"href\"))\n",
    "\n",
    "#remove first three (not what I'm seeking)\n",
    "episodes_url = episodes_url[3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#seasons and how many episodes in each season\n",
    "seasons = {\"s1\":22, \"s2\":24, \"s3\":24, \"s4\":16, \"s5\":24, \"s6\":22, \"s7\":23, \"s8\":22}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#clean unnecessary part\n",
    "def clean(text):\n",
    "    text = re.sub('<br/>','\\\\n',text)\n",
    "    text = re.sub('Written by:.*?\\\\n','',text)\n",
    "    text = re.sub('Originally Aired:.*?\\\\n','',text)\n",
    "    text = re.sub('Directed by:.*?\\\\n','',text)\n",
    "    text = re.sub('Transcribed by:.*?\\\\n','',text)\n",
    "    text = re.sub('Betaed by:.*?\\\\n','',text)\n",
    "    text = re.sub('DISCLAIMER:.*?\\\\n','',text)\n",
    "    text = re.sub('\\[.*?\\]','',text)\n",
    "    text = re.sub('\\(.*?\\)','',text)\n",
    "    text = re.sub('<.*?>','',text)\n",
    "    return text\n",
    "\n",
    "#put in on txt named after its season and episode number\n",
    "counter = 0\n",
    "for season,eps in sorted(seasons.iteritems()):\n",
    "    for i in range(1,int(eps)+1):\n",
    "        eps_url = episodes_url[counter]\n",
    "        eps_page = urllib2.urlopen(eps_url)\n",
    "        eps_soup = BeautifulSoup(eps_page, \"lxml\")\n",
    "        \n",
    "        eps_entrytext = eps_soup.findAll(\"div\", { \"class\" : \"entryText\" })\n",
    "        clean_eps = clean(str(eps_entrytext[0]))\n",
    "        \n",
    "        dialogue_pattern = re.compile('\\\\n(.*?)\\\\n')\n",
    "        dialogues = dialogue_pattern.findall(clean_eps)\n",
    "        dialogues = [unidecode(s.decode('utf8')) for s in dialogues if s]\n",
    "        \n",
    "        output_name = season + \"e\" + str(i) + \".txt\"\n",
    "        with open(output_name, \"w\") as output:\n",
    "            for dialogue in dialogues:\n",
    "                output.write(\"%s\\n\" % dialogue)\n",
    "                \n",
    "        counter = counter+1                  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
